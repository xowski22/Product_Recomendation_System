{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-28T13:23:18.071857Z",
     "start_time": "2025-01-28T13:23:16.670115Z"
    }
   },
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from src.models.model import MatrixFactorization\n",
    "from src.data.preprocessing import load_ml1m_data, preprocess_ratings, split_data\n",
    "from src.data.dataset import RecommenderDataset\n",
    "from src.training.trainer import train_model\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:23:18.075600Z",
     "start_time": "2025-01-28T13:23:18.074075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hyperparameter_grind = {\n",
    "    'embedding_dim' : [50, 100, 150],\n",
    "    'reg_lambda' : [0.001, 0.01, 0.1],\n",
    "    'dropout' : [0.1, 0.2, 0.3]\n",
    "}"
   ],
   "id": "d914ae10f4864594",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:23:18.120420Z",
     "start_time": "2025-01-28T13:23:18.118577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(config):\n",
    "\n",
    "    ratings_df, _ = load_ml1m_data('../data/raw/ml-1m')\n",
    "    processed_df, user_mapping, item_mapping = preprocess_ratings(ratings_df)\n",
    "    train_data, val_data = split_data(processed_df)\n",
    "\n",
    "    train_dataset = RecommenderDataset(train_data)\n",
    "    val_dataset = RecommenderDataset(val_data)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, len(user_mapping), len(item_mapping)"
   ],
   "id": "e945fe12b3d6667d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:23:18.164185Z",
     "start_time": "2025-01-28T13:23:18.161868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_hyperparameter_experiment(config, hyperparams, train_loader, val_loader):\n",
    "\n",
    "    model = MatrixFactorization(\n",
    "        num_users=config['num_users'],\n",
    "        n_items=config['n_items'],\n",
    "        embedding_dim=hyperparams['embedding_dim'],\n",
    "        reg_lambda=hyperparams['reg_lambda'],\n",
    "    )\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, config)\n",
    "\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    metrics = client.get_run(mlflow.active_run().info.run_id).data.metrics\n",
    "\n",
    "    epochs = range(config['training']['num_epochs'])\n",
    "    train_losses = [metrics.get(f'train_loss_{i}', 0) for i in epochs]\n",
    "    val_losses = [metrics.get(f'val_loss_{i}', 0) for i in epochs]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='TrainLoss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss Curves (dim={hyperparams[\"embedding_dim\"]}, lambda={hyperparams[\"reg_lambda\"]}, dropout={hyperparams[\"dropout\"]})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig('loss_plot.png')\n",
    "    mlflow.log_artifact('loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        'model': trained_model,\n",
    "        'params': hyperparams,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }\n"
   ],
   "id": "cd868b72f22c7d15",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:23:18.207295Z",
     "start_time": "2025-01-28T13:23:18.205081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiments(config):\n",
    "\n",
    "    train_loader, val_loader, num_users, n_items = prepare_data(config)\n",
    "\n",
    "    config.update({\n",
    "        'num_users': num_users,\n",
    "        'n_items': n_items\n",
    "    })\n",
    "\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        mlflow.end_run()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Hyperparameter_optimization\") as parent_run:\n",
    "        for params in product(*hyperparameter_grind.values()):\n",
    "            hyperparams = dict(zip(hyperparameter_grind.keys(), params))\n",
    "            print(f\"Running experiment with parameters: {hyperparams}\")\n",
    "\n",
    "            with mlflow.start_run(nested=True) as child_run:\n",
    "\n",
    "                mlflow.log_params({\n",
    "                    \"embedding_dim\": hyperparams['embedding_dim'],\n",
    "                    \"reg_lambda\": hyperparams['reg_lambda'],\n",
    "                    \"dropout\": hyperparams['dropout'],\n",
    "                    \"batch_size\": config['training']['batch_size'],\n",
    "                    \"learning_rate\": config['training']['learning_rate'],\n",
    "                    \"num_epochs\": config['training']['num_epochs']\n",
    "                })\n",
    "\n",
    "                model = run_hyperparameter_experiment(\n",
    "                    config,\n",
    "                    hyperparams,\n",
    "                    train_loader,\n",
    "                    val_loader\n",
    "                )\n",
    "\n",
    "                mlflow.log_params(hyperparams)\n",
    "\n",
    "                results.append({\n",
    "                    'params': hyperparams,\n",
    "                    'model': model,\n",
    "                })\n",
    "\n",
    "        return results"
   ],
   "id": "fd2cbcb0913b5d58",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:45:24.658537Z",
     "start_time": "2025-01-28T13:23:18.256197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    with open('../config/config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    results = run_experiments(config)"
   ],
   "id": "bc465cce29698156",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with parameters: {'embedding_dim': 50, 'reg_lambda': 0.001, 'dropout': 0.1}\n",
      "Epoch 1/15 \n",
      "Train Loss: 94.71642955605158\n",
      "Val Loss: 0.8643854061953166\n",
      "----------------------------------------\n",
      "Epoch 2/15 \n",
      "Train Loss: 0.8385179592525159\n",
      "Val Loss: 0.8322409070165433\n",
      "----------------------------------------\n",
      "Epoch 3/15 \n",
      "Train Loss: 0.8212521912278742\n",
      "Val Loss: 0.827954139205331\n",
      "----------------------------------------\n",
      "Epoch 4/15 \n",
      "Train Loss: 0.8173863214725858\n",
      "Val Loss: 0.8275419940703661\n",
      "----------------------------------------\n",
      "Epoch 5/15 \n",
      "Train Loss: 0.8162641935178607\n",
      "Val Loss: 0.8277309584411687\n",
      "----------------------------------------\n",
      "Epoch 6/15 \n",
      "Train Loss: 0.8157287944219384\n",
      "Val Loss: 0.8275303229575194\n",
      "----------------------------------------\n",
      "Epoch 7/15 \n",
      "Train Loss: 0.8156290744353626\n",
      "Val Loss: 0.8277419905678172\n",
      "----------------------------------------\n",
      "Epoch 8/15 \n",
      "Train Loss: 0.815471697264096\n",
      "Val Loss: 0.8279702814625992\n",
      "----------------------------------------\n",
      "Epoch 9/15 \n",
      "Train Loss: 0.8085843332622524\n",
      "Val Loss: 0.8271427094574091\n",
      "----------------------------------------\n",
      "Epoch 10/15 \n",
      "Train Loss: 0.8080800868607154\n",
      "Val Loss: 0.8268692453921566\n",
      "----------------------------------------\n",
      "Epoch 11/15 \n",
      "Train Loss: 0.8078999947469006\n",
      "Val Loss: 0.8267726185597523\n",
      "----------------------------------------\n",
      "Epoch 12/15 \n",
      "Train Loss: 0.8078032881748501\n",
      "Val Loss: 0.826745750488605\n",
      "----------------------------------------\n",
      "Epoch 13/15 \n",
      "Train Loss: 0.8077698613008308\n",
      "Val Loss: 0.8266946367504729\n",
      "----------------------------------------\n",
      "Epoch 14/15 \n",
      "Train Loss: 0.8077240201279534\n",
      "Val Loss: 0.8266852208039582\n",
      "----------------------------------------\n",
      "Epoch 15/15 \n",
      "Train Loss: 0.8077179577544548\n",
      "Val Loss: 0.8266850541216474\n",
      "----------------------------------------\n",
      "Running experiment with parameters: {'embedding_dim': 50, 'reg_lambda': 0.001, 'dropout': 0.2}\n",
      "Epoch 1/15 \n",
      "Train Loss: 95.03965571008091\n",
      "Val Loss: 0.8646840682147179\n",
      "----------------------------------------\n",
      "Epoch 2/15 \n",
      "Train Loss: 0.8385980691703274\n",
      "Val Loss: 0.832323306481463\n",
      "----------------------------------------\n",
      "Epoch 3/15 \n",
      "Train Loss: 0.8212638319248983\n",
      "Val Loss: 0.8281555472231414\n",
      "----------------------------------------\n",
      "Epoch 4/15 \n",
      "Train Loss: 0.817420171445192\n",
      "Val Loss: 0.8271492652247063\n",
      "----------------------------------------\n",
      "Epoch 5/15 \n",
      "Train Loss: 0.816335119986967\n",
      "Val Loss: 0.8273685065003366\n",
      "----------------------------------------\n",
      "Epoch 6/15 \n",
      "Train Loss: 0.8158497077235697\n",
      "Val Loss: 0.8275711411277759\n",
      "----------------------------------------\n",
      "Epoch 7/15 \n",
      "Train Loss: 0.8155268979584571\n",
      "Val Loss: 0.8279780964352196\n",
      "----------------------------------------\n",
      "Epoch 8/15 \n",
      "Train Loss: 0.8155563266470099\n",
      "Val Loss: 0.8277714775156608\n",
      "----------------------------------------\n",
      "Epoch 9/15 \n",
      "Train Loss: 0.808630537521474\n",
      "Val Loss: 0.8269533645826429\n",
      "----------------------------------------\n",
      "Epoch 10/15 \n",
      "Train Loss: 0.8080910957094136\n",
      "Val Loss: 0.8267432050690083\n",
      "----------------------------------------\n",
      "Epoch 11/15 \n",
      "Train Loss: 0.8079076765690156\n",
      "Val Loss: 0.8266517493065855\n",
      "----------------------------------------\n",
      "Epoch 12/15 \n",
      "Train Loss: 0.8078454226090948\n",
      "Val Loss: 0.8266370807674857\n",
      "----------------------------------------\n",
      "Epoch 13/15 \n",
      "Train Loss: 0.807774897950521\n",
      "Val Loss: 0.8266255173090934\n",
      "----------------------------------------\n",
      "Epoch 14/15 \n",
      "Train Loss: 0.807749742685695\n",
      "Val Loss: 0.8266542937251443\n",
      "----------------------------------------\n",
      "Epoch 15/15 \n",
      "Train Loss: 0.8077236676248543\n",
      "Val Loss: 0.8266462866819904\n",
      "----------------------------------------\n",
      "Running experiment with parameters: {'embedding_dim': 50, 'reg_lambda': 0.001, 'dropout': 0.3}\n",
      "Epoch 1/15 \n",
      "Train Loss: 95.21848181576564\n",
      "Val Loss: 0.8652909261842454\n",
      "----------------------------------------\n",
      "Epoch 2/15 \n",
      "Train Loss: 0.8386980814737558\n",
      "Val Loss: 0.8326973678902869\n",
      "----------------------------------------\n",
      "Epoch 3/15 \n",
      "Train Loss: 0.8213270768936773\n",
      "Val Loss: 0.8280430834499675\n",
      "----------------------------------------\n",
      "Epoch 4/15 \n",
      "Train Loss: 0.8174440352156631\n",
      "Val Loss: 0.8272731460139908\n",
      "----------------------------------------\n",
      "Epoch 5/15 \n",
      "Train Loss: 0.8162884737318691\n",
      "Val Loss: 0.8273173979485332\n",
      "----------------------------------------\n",
      "Epoch 6/15 \n",
      "Train Loss: 0.8157601689139643\n",
      "Val Loss: 0.8276644659562898\n",
      "----------------------------------------\n",
      "Epoch 7/15 \n",
      "Train Loss: 0.8155401749252597\n",
      "Val Loss: 0.8281941906389944\n",
      "----------------------------------------\n",
      "Epoch 8/15 \n",
      "Train Loss: 0.8154044465967\n",
      "Val Loss: 0.8279001806610605\n",
      "----------------------------------------\n",
      "Epoch 9/15 \n",
      "Train Loss: 0.808636913008855\n",
      "Val Loss: 0.8271031671621368\n",
      "----------------------------------------\n",
      "Epoch 10/15 \n",
      "Train Loss: 0.8080984729700524\n",
      "Val Loss: 0.8267913841983865\n",
      "----------------------------------------\n",
      "Epoch 11/15 \n",
      "Train Loss: 0.8078887928445597\n",
      "Val Loss: 0.8266855970082265\n",
      "----------------------------------------\n",
      "Epoch 12/15 \n",
      "Train Loss: 0.8078080306580036\n",
      "Val Loss: 0.826755135774765\n",
      "----------------------------------------\n",
      "Epoch 13/15 \n",
      "Train Loss: 0.8077489720014042\n",
      "Val Loss: 0.8266533933727694\n",
      "----------------------------------------\n",
      "Epoch 14/15 \n",
      "Train Loss: 0.8077349271677422\n",
      "Val Loss: 0.8266554334113327\n",
      "----------------------------------------\n",
      "Epoch 15/15 \n",
      "Train Loss: 0.8077314334192972\n",
      "Val Loss: 0.8266691083537793\n",
      "----------------------------------------\n",
      "Running experiment with parameters: {'embedding_dim': 50, 'reg_lambda': 0.01, 'dropout': 0.1}\n",
      "Epoch 1/15 \n",
      "Train Loss: 94.81531575865853\n",
      "Val Loss: 0.8643965556435835\n",
      "----------------------------------------\n",
      "Epoch 2/15 \n",
      "Train Loss: 0.8474956309448326\n",
      "Val Loss: 0.8325603554836848\n",
      "----------------------------------------\n",
      "Epoch 3/15 \n",
      "Train Loss: 0.830240973413329\n",
      "Val Loss: 0.8280239698506286\n",
      "----------------------------------------\n",
      "Epoch 4/15 \n",
      "Train Loss: 0.8265384426262249\n",
      "Val Loss: 0.8272984919882477\n",
      "----------------------------------------\n",
      "Epoch 5/15 \n",
      "Train Loss: 0.8252162863581236\n",
      "Val Loss: 0.8276480938321645\n",
      "----------------------------------------\n",
      "Epoch 6/15 \n",
      "Train Loss: 0.8248352349986746\n",
      "Val Loss: 0.8277529982357779\n",
      "----------------------------------------\n",
      "Epoch 7/15 \n",
      "Train Loss: 0.8245835279995066\n",
      "Val Loss: 0.8279730316182397\n",
      "----------------------------------------\n",
      "Epoch 8/15 \n",
      "Train Loss: 0.8244158134149244\n",
      "Val Loss: 0.8278990255827257\n",
      "----------------------------------------\n",
      "Epoch 9/15 \n",
      "Train Loss: 0.8176215352925703\n",
      "Val Loss: 0.8270989738867135\n",
      "----------------------------------------\n",
      "Epoch 10/15 \n",
      "Train Loss: 0.8171080963607599\n",
      "Val Loss: 0.8268246249851705\n",
      "----------------------------------------\n",
      "Epoch 11/15 \n",
      "Train Loss: 0.8169144947776925\n",
      "Val Loss: 0.8266697193538235\n",
      "----------------------------------------\n",
      "Epoch 12/15 \n",
      "Train Loss: 0.8168195279147218\n",
      "Val Loss: 0.8266457677249792\n",
      "----------------------------------------\n",
      "Epoch 13/15 \n",
      "Train Loss: 0.8167634250821566\n",
      "Val Loss: 0.8266254424648413\n",
      "----------------------------------------\n",
      "Epoch 14/15 \n",
      "Train Loss: 0.8167516066509752\n",
      "Val Loss: 0.8266033096404619\n",
      "----------------------------------------\n",
      "Epoch 15/15 \n",
      "Train Loss: 0.8167302824731427\n",
      "Val Loss: 0.8266026319474725\n",
      "----------------------------------------\n",
      "Running experiment with parameters: {'embedding_dim': 50, 'reg_lambda': 0.01, 'dropout': 0.2}\n",
      "Epoch 1/15 \n",
      "Train Loss: 94.70584895411682\n",
      "Val Loss: 0.8651011873582427\n",
      "----------------------------------------\n",
      "Epoch 2/15 \n",
      "Train Loss: 0.8477381613672538\n",
      "Val Loss: 0.8323131384743915\n",
      "----------------------------------------\n",
      "Epoch 3/15 \n",
      "Train Loss: 0.8302892749189215\n",
      "Val Loss: 0.8279140064837226\n",
      "----------------------------------------\n",
      "Epoch 4/15 \n",
      "Train Loss: 0.8264607300255324\n",
      "Val Loss: 0.827514306659395\n",
      "----------------------------------------\n",
      "Epoch 5/15 \n",
      "Train Loss: 0.825258525714277\n",
      "Val Loss: 0.8271559430995357\n",
      "----------------------------------------\n",
      "Epoch 6/15 \n",
      "Train Loss: 0.8248231713937529\n",
      "Val Loss: 0.8274344594020608\n",
      "----------------------------------------\n",
      "Epoch 7/15 \n",
      "Train Loss: 0.8245502282650596\n",
      "Val Loss: 0.8278928889213124\n",
      "----------------------------------------\n",
      "Epoch 8/15 \n",
      "Train Loss: 0.8244102120799922\n",
      "Val Loss: 0.8280341098238777\n",
      "----------------------------------------\n",
      "Epoch 9/15 \n",
      "Train Loss: 0.8244914244167673\n",
      "Val Loss: 0.8278942748294071\n",
      "----------------------------------------\n",
      "Epoch 10/15 \n",
      "Train Loss: 0.817519486203133\n",
      "Val Loss: 0.8271314762365871\n",
      "----------------------------------------\n",
      "Epoch 11/15 \n",
      "Train Loss: 0.8170476914891851\n",
      "Val Loss: 0.8269000188879531\n",
      "----------------------------------------\n",
      "Epoch 12/15 \n",
      "Train Loss: 0.8168519339108194\n",
      "Val Loss: 0.8268151581239715\n",
      "----------------------------------------\n",
      "Epoch 13/15 \n",
      "Train Loss: 0.8167813699321538\n",
      "Val Loss: 0.8267740487947498\n",
      "----------------------------------------\n",
      "Epoch 14/15 \n",
      "Train Loss: 0.8167348827355997\n",
      "Val Loss: 0.8267463650637839\n",
      "----------------------------------------\n",
      "Epoch 15/15 \n",
      "Train Loss: 0.8167152971966042\n",
      "Val Loss: 0.8267762211990982\n",
      "----------------------------------------\n",
      "Running experiment with parameters: {'embedding_dim': 50, 'reg_lambda': 0.01, 'dropout': 0.3}\n",
      "Epoch 1/15 \n",
      "Train Loss: 94.93378804126502\n",
      "Val Loss: 0.8647711579607453\n",
      "----------------------------------------\n",
      "Epoch 2/15 \n",
      "Train Loss: 0.8476531777674223\n",
      "Val Loss: 0.8325421214771057\n",
      "----------------------------------------\n",
      "Epoch 3/15 \n",
      "Train Loss: 0.8303144708076763\n",
      "Val Loss: 0.8280960706587563\n",
      "----------------------------------------\n",
      "Epoch 4/15 \n",
      "Train Loss: 0.8264874186452309\n",
      "Val Loss: 0.8272154106164467\n",
      "----------------------------------------\n",
      "Epoch 5/15 \n",
      "Train Loss: 0.8252153041233475\n",
      "Val Loss: 0.8274291261830394\n",
      "----------------------------------------\n",
      "Epoch 6/15 \n",
      "Train Loss: 0.8247951354530824\n",
      "Val Loss: 0.8276545094303496\n",
      "----------------------------------------\n",
      "Epoch 7/15 \n",
      "Train Loss: 0.8246307430372785\n",
      "Val Loss: 0.8280432764071345\n",
      "----------------------------------------\n",
      "Epoch 8/15 \n",
      "Train Loss: 0.8244754577039634\n",
      "Val Loss: 0.8282741844005792\n",
      "----------------------------------------\n",
      "Epoch 9/15 \n",
      "Train Loss: 0.8176255725287804\n",
      "Val Loss: 0.8272788163110071\n",
      "----------------------------------------\n",
      "Epoch 10/15 \n",
      "Train Loss: 0.817112310863939\n",
      "Val Loss: 0.8269463960684345\n",
      "----------------------------------------\n",
      "Epoch 11/15 \n",
      "Train Loss: 0.8169005821607003\n",
      "Val Loss: 0.8268112045160411\n",
      "----------------------------------------\n",
      "Epoch 12/15 \n",
      "Train Loss: 0.8168270500204158\n",
      "Val Loss: 0.8267523458493267\n",
      "----------------------------------------\n",
      "Epoch 13/15 \n",
      "Train Loss: 0.8167739249888033\n",
      "Val Loss: 0.8267187372955922\n",
      "----------------------------------------\n",
      "Epoch 14/15 \n",
      "Train Loss: 0.8167213646138295\n",
      "Val Loss: 0.8267846178842597\n",
      "----------------------------------------\n",
      "Epoch 15/15 \n",
      "Train Loss: 0.8167257221624354\n",
      "Val Loss: 0.8267471235929509\n",
      "----------------------------------------\n",
      "Running experiment with parameters: {'embedding_dim': 50, 'reg_lambda': 0.1, 'dropout': 0.1}\n",
      "Epoch 1/15 \n",
      "Train Loss: 95.32160860530321\n",
      "Val Loss: 0.8646709277930354\n",
      "----------------------------------------\n",
      "Epoch 2/15 \n",
      "Train Loss: 0.9376505930800538\n",
      "Val Loss: 0.8324561595239856\n",
      "----------------------------------------\n",
      "Epoch 3/15 \n",
      "Train Loss: 0.9203424707590213\n",
      "Val Loss: 0.8281452145501351\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../config/config.yaml\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      3\u001B[0m     config \u001B[38;5;241m=\u001B[39m yaml\u001B[38;5;241m.\u001B[39msafe_load(f)\n\u001B[0;32m----> 5\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mrun_experiments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 33\u001B[0m, in \u001B[0;36mrun_experiments\u001B[0;34m(config)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mstart_run(nested\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m child_run:\n\u001B[1;32m     24\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mlog_params({\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding_dim\u001B[39m\u001B[38;5;124m\"\u001B[39m: hyperparams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding_dim\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     26\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreg_lambda\u001B[39m\u001B[38;5;124m\"\u001B[39m: hyperparams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreg_lambda\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_epochs\u001B[39m\u001B[38;5;124m\"\u001B[39m: config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     31\u001B[0m     })\n\u001B[0;32m---> 33\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mrun_hyperparameter_experiment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhyperparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m        \u001B[49m\u001B[43mval_loader\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mlog_params(hyperparams)\n\u001B[1;32m     42\u001B[0m     results\u001B[38;5;241m.\u001B[39mappend({\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m: hyperparams,\n\u001B[1;32m     44\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m: model,\n\u001B[1;32m     45\u001B[0m     })\n",
      "Cell \u001B[0;32mIn[4], line 10\u001B[0m, in \u001B[0;36mrun_hyperparameter_experiment\u001B[0;34m(config, hyperparams, train_loader, val_loader)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrun_hyperparameter_experiment\u001B[39m(config, hyperparams, train_loader, val_loader):\n\u001B[1;32m      3\u001B[0m     model \u001B[38;5;241m=\u001B[39m MatrixFactorization(\n\u001B[1;32m      4\u001B[0m         num_users\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_users\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m      5\u001B[0m         n_items\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_items\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m      6\u001B[0m         embedding_dim\u001B[38;5;241m=\u001B[39mhyperparams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding_dim\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m      7\u001B[0m         reg_lambda\u001B[38;5;241m=\u001B[39mhyperparams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreg_lambda\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m      8\u001B[0m     )\n\u001B[0;32m---> 10\u001B[0m     trained_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     client \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mtracking\u001B[38;5;241m.\u001B[39mMlflowClient()\n\u001B[1;32m     13\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget_run(mlflow\u001B[38;5;241m.\u001B[39mactive_run()\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mrun_id)\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mmetrics\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/src/training/trainer.py:65\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, val_loader, config)\u001B[0m\n\u001B[1;32m     61\u001B[0m         mlflow\u001B[38;5;241m.\u001B[39mlog_param(key, value)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m]):\n\u001B[0;32m---> 65\u001B[0m     train_metrics \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     val_metrics \u001B[38;5;241m=\u001B[39m validate(model, val_loader, criterion, device)\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/src/training/trainer.py:32\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, train_loader, optimizer, criterion, device)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m#backward pass - updating weights\u001B[39;00m\n\u001B[1;32m     31\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 32\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     35\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    647\u001B[0m     )\n\u001B[0;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:815\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    813\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    814\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 815\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    816\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    817\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    819\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
