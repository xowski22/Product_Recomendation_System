{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-29T11:39:58.534069Z",
     "start_time": "2025-01-29T11:39:57.113233Z"
    }
   },
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from src.models.model import MatrixFactorization\n",
    "from src.data.preprocessing import load_ml1m_data, preprocess_ratings, split_data\n",
    "from src.data.dataset import RecommenderDataset\n",
    "from src.training.trainer import train_model\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T11:39:58.538084Z",
     "start_time": "2025-01-29T11:39:58.536382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hyperparameter_grind = {\n",
    "    'embedding_dim' : [50, 100, 150],\n",
    "    'reg_lambda' : [0.001, 0.01, 0.1],\n",
    "    'dropout' : [0.1, 0.2, 0.3]\n",
    "}"
   ],
   "id": "d914ae10f4864594",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T11:39:58.584393Z",
     "start_time": "2025-01-29T11:39:58.582492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(config):\n",
    "\n",
    "    ratings_df, _ = load_ml1m_data('../data/raw/ml-1m')\n",
    "    processed_df, user_mapping, item_mapping = preprocess_ratings(ratings_df)\n",
    "    train_data, val_data = split_data(processed_df)\n",
    "\n",
    "    train_dataset = RecommenderDataset(train_data)\n",
    "    val_dataset = RecommenderDataset(val_data)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, len(user_mapping), len(item_mapping)"
   ],
   "id": "e945fe12b3d6667d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T11:39:58.629614Z",
     "start_time": "2025-01-29T11:39:58.626730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_hyperparameter_experiment(config, hyperparams, train_loader, val_loader):\n",
    "\n",
    "    model = MatrixFactorization(\n",
    "        num_users=config['num_users'],\n",
    "        n_items=config['n_items'],\n",
    "        embedding_dim=hyperparams['embedding_dim'],\n",
    "        reg_lambda=hyperparams['reg_lambda'],\n",
    "    )\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, config)\n",
    "\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    current_run = mlflow.active_run()\n",
    "    metrics = client.get_run(current_run.info.run_id).data.metrics\n",
    "\n",
    "    epochs = range(config['training']['num_epochs'])\n",
    "    train_losses = [metrics.get(f'train_loss_{i}', 0) for i in epochs]\n",
    "    val_losses = [metrics.get(f'val_loss_{i}', 0) for i in epochs]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='TrainLoss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss Curves (dim={hyperparams[\"embedding_dim\"]}, lambda={hyperparams[\"reg_lambda\"]}, dropout={hyperparams[\"dropout\"]})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plot_path = f\"mlruns/0/{current_run.info.run_id}/loss_plot_{hyperparams['embedding_dim']}_{hyperparams['reg_lambda']}_{hyperparams['dropout']}.png\"\n",
    "\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        'model': trained_model,\n",
    "        'params': hyperparams,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }\n"
   ],
   "id": "cd868b72f22c7d15",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T11:39:58.674799Z",
     "start_time": "2025-01-29T11:39:58.672313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_experiments(config):\n",
    "\n",
    "    train_loader, val_loader, num_users, n_items = prepare_data(config)\n",
    "\n",
    "    config.update({\n",
    "        'num_users': num_users,\n",
    "        'n_items': n_items\n",
    "    })\n",
    "\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        mlflow.end_run()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    with mlflow.start_run(run_name=\"Hyperparameter_optimization\") as parent_run:\n",
    "        for params in product(*hyperparameter_grind.values()):\n",
    "            hyperparams = dict(zip(hyperparameter_grind.keys(), params))\n",
    "            print(f\"Running experiment with parameters: {hyperparams}\")\n",
    "\n",
    "            with mlflow.start_run(nested=True) as child_run:\n",
    "\n",
    "                mlflow.log_params({\n",
    "                    \"embedding_dim\": hyperparams['embedding_dim'],\n",
    "                    \"reg_lambda\": hyperparams['reg_lambda'],\n",
    "                    \"dropout\": hyperparams['dropout'],\n",
    "                    \"batch_size\": config['training']['batch_size'],\n",
    "                    \"learning_rate\": config['training']['learning_rate'],\n",
    "                    \"num_epochs\": config['training']['num_epochs']\n",
    "                })\n",
    "\n",
    "                model = run_hyperparameter_experiment(\n",
    "                    config,\n",
    "                    hyperparams,\n",
    "                    train_loader,\n",
    "                    val_loader\n",
    "                )\n",
    "\n",
    "                mlflow.log_params(hyperparams)\n",
    "\n",
    "                results.append({\n",
    "                    'params': hyperparams,\n",
    "                    'model': model,\n",
    "                })\n",
    "\n",
    "        return results"
   ],
   "id": "fd2cbcb0913b5d58",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T11:45:39.934937Z",
     "start_time": "2025-01-29T11:39:58.720118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    with open('../config/config.yaml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    results = run_experiments(config)"
   ],
   "id": "bc465cce29698156",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with parameters: {'embedding_dim': 50, 'reg_lambda': 0.001, 'dropout': 0.1}\n",
      "Epoch 1/15 \n",
      "Train Loss: 95.05388364508077\n",
      "Val Loss: 0.8647248819136726\n",
      "----------------------------------------\n",
      "Epoch 2/15 \n",
      "Train Loss: 0.8388159650712455\n",
      "Val Loss: 0.832718297367247\n",
      "----------------------------------------\n",
      "Epoch 3/15 \n",
      "Train Loss: 0.8212985032764919\n",
      "Val Loss: 0.8281270281881837\n",
      "----------------------------------------\n",
      "Epoch 4/15 \n",
      "Train Loss: 0.8174495252620542\n",
      "Val Loss: 0.8275914346407182\n",
      "----------------------------------------\n",
      "Epoch 5/15 \n",
      "Train Loss: 0.8163062832116756\n",
      "Val Loss: 0.8275716483249774\n",
      "----------------------------------------\n",
      "Epoch 6/15 \n",
      "Train Loss: 0.8158085202585361\n",
      "Val Loss: 0.8274392001640698\n",
      "----------------------------------------\n",
      "Epoch 7/15 \n",
      "Train Loss: 0.8155343811468898\n",
      "Val Loss: 0.8278301787763471\n",
      "----------------------------------------\n",
      "Epoch 8/15 \n",
      "Train Loss: 0.8154869116988743\n",
      "Val Loss: 0.8279825413953549\n",
      "----------------------------------------\n",
      "Epoch 9/15 \n",
      "Train Loss: 0.8153984434512351\n",
      "Val Loss: 0.827865711396044\n",
      "----------------------------------------\n",
      "Epoch 10/15 \n",
      "Train Loss: 0.8153434009263298\n",
      "Val Loss: 0.8281835505139423\n",
      "----------------------------------------\n",
      "Epoch 11/15 \n",
      "Train Loss: 0.8085714088200017\n",
      "Val Loss: 0.8272124358899152\n",
      "----------------------------------------\n",
      "Epoch 12/15 \n",
      "Train Loss: 0.8080596540090151\n",
      "Val Loss: 0.8270289880758055\n",
      "----------------------------------------\n",
      "Epoch 13/15 \n",
      "Train Loss: 0.8078442635793052\n",
      "Val Loss: 0.826879825969766\n",
      "----------------------------------------\n",
      "Epoch 14/15 \n",
      "Train Loss: 0.8077609493314272\n",
      "Val Loss: 0.8267992452282747\n",
      "----------------------------------------\n",
      "Epoch 15/15 \n",
      "Train Loss: 0.8077101115583373\n",
      "Val Loss: 0.8268413187808435\n",
      "----------------------------------------\n",
      "Running experiment with parameters: {'embedding_dim': 50, 'reg_lambda': 0.001, 'dropout': 0.2}\n",
      "Epoch 1/15 \n",
      "Train Loss: 94.89648568337034\n",
      "Val Loss: 0.8645913889268158\n",
      "----------------------------------------\n",
      "Epoch 2/15 \n",
      "Train Loss: 0.8385751244331144\n",
      "Val Loss: 0.83229775252494\n",
      "----------------------------------------\n",
      "Epoch 3/15 \n",
      "Train Loss: 0.8212221001926578\n",
      "Val Loss: 0.8282660470032494\n",
      "----------------------------------------\n",
      "Epoch 4/15 \n",
      "Train Loss: 0.8174997934086708\n",
      "Val Loss: 0.8271603803087791\n",
      "----------------------------------------\n",
      "Epoch 5/15 \n",
      "Train Loss: 0.8162338140773018\n",
      "Val Loss: 0.8270525106360572\n",
      "----------------------------------------\n",
      "Epoch 6/15 \n",
      "Train Loss: 0.8157852041829083\n",
      "Val Loss: 0.8277520045151866\n",
      "----------------------------------------\n",
      "Epoch 7/15 \n",
      "Train Loss: 0.8155624764323911\n",
      "Val Loss: 0.8277274751398171\n",
      "----------------------------------------\n",
      "Epoch 8/15 \n",
      "Train Loss: 0.8155238508753878\n",
      "Val Loss: 0.8279681005541971\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../config/config.yaml\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      3\u001B[0m     config \u001B[38;5;241m=\u001B[39m yaml\u001B[38;5;241m.\u001B[39msafe_load(f)\n\u001B[0;32m----> 5\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mrun_experiments\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 33\u001B[0m, in \u001B[0;36mrun_experiments\u001B[0;34m(config)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mlflow\u001B[38;5;241m.\u001B[39mstart_run(nested\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m child_run:\n\u001B[1;32m     24\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mlog_params({\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membedding_dim\u001B[39m\u001B[38;5;124m\"\u001B[39m: hyperparams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding_dim\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     26\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreg_lambda\u001B[39m\u001B[38;5;124m\"\u001B[39m: hyperparams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreg_lambda\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_epochs\u001B[39m\u001B[38;5;124m\"\u001B[39m: config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     31\u001B[0m     })\n\u001B[0;32m---> 33\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mrun_hyperparameter_experiment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhyperparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m        \u001B[49m\u001B[43mval_loader\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m     mlflow\u001B[38;5;241m.\u001B[39mlog_params(hyperparams)\n\u001B[1;32m     42\u001B[0m     results\u001B[38;5;241m.\u001B[39mappend({\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m: hyperparams,\n\u001B[1;32m     44\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m: model,\n\u001B[1;32m     45\u001B[0m     })\n",
      "Cell \u001B[0;32mIn[4], line 10\u001B[0m, in \u001B[0;36mrun_hyperparameter_experiment\u001B[0;34m(config, hyperparams, train_loader, val_loader)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrun_hyperparameter_experiment\u001B[39m(config, hyperparams, train_loader, val_loader):\n\u001B[1;32m      3\u001B[0m     model \u001B[38;5;241m=\u001B[39m MatrixFactorization(\n\u001B[1;32m      4\u001B[0m         num_users\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_users\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m      5\u001B[0m         n_items\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_items\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m      6\u001B[0m         embedding_dim\u001B[38;5;241m=\u001B[39mhyperparams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedding_dim\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m      7\u001B[0m         reg_lambda\u001B[38;5;241m=\u001B[39mhyperparams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreg_lambda\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m      8\u001B[0m     )\n\u001B[0;32m---> 10\u001B[0m     trained_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     client \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mtracking\u001B[38;5;241m.\u001B[39mMlflowClient()\n\u001B[1;32m     13\u001B[0m     current_run \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mactive_run()\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/src/training/trainer.py:65\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, val_loader, config)\u001B[0m\n\u001B[1;32m     61\u001B[0m         mlflow\u001B[38;5;241m.\u001B[39mlog_param(key, value)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m]):\n\u001B[0;32m---> 65\u001B[0m     train_metrics \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     val_metrics \u001B[38;5;241m=\u001B[39m validate(model, val_loader, criterion, device)\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/src/training/trainer.py:22\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, train_loader, optimizer, criterion, device)\u001B[0m\n\u001B[1;32m     19\u001B[0m ratings \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrating\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m#redicting ratings by foward pass\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m predictions, user_embeds, item_embeds \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitem_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m#calculating mse loss\u001B[39;00m\n\u001B[1;32m     24\u001B[0m mse_loss \u001B[38;5;241m=\u001B[39m criterion(predictions, ratings)\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1727\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1725\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1726\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1727\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1738\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1733\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1734\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1736\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1737\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1740\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1741\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/src/models/model.py:36\u001B[0m, in \u001B[0;36mMatrixFactorization.forward\u001B[0;34m(self, users_ids, items_ids)\u001B[0m\n\u001B[1;32m     33\u001B[0m item_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitem_embeddings(items_ids))\n\u001B[1;32m     35\u001B[0m user_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_bn(user_embeds)\n\u001B[0;32m---> 36\u001B[0m item_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem_bn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem_embeds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m#get bias for users and items\u001B[39;00m\n\u001B[1;32m     39\u001B[0m user_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_bias[users_ids]\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1727\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1725\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1726\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1727\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1738\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1733\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1734\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1736\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1737\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1740\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1741\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/.venv/lib/python3.13/site-packages/torch/nn/modules/batchnorm.py:200\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    186\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    188\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mbatch_norm(\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;00m\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrack_running_stats\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrack_running_stats \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m--> 200\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m,\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias,\n\u001B[1;32m    202\u001B[0m     bn_training,\n\u001B[1;32m    203\u001B[0m     exponential_average_factor,\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meps,\n\u001B[1;32m    205\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/Product_Recomendation_System/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1903\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1898\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;241m=\u001B[39m OrderedDict()\n\u001B[1;32m   1900\u001B[0m \u001B[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001B[39;00m\n\u001B[1;32m   1901\u001B[0m \u001B[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001B[39;00m\n\u001B[1;32m   1902\u001B[0m \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001B[39;00m\n\u001B[0;32m-> 1903\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tensor, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModule\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m   1904\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m:\n\u001B[1;32m   1905\u001B[0m         _parameters \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
